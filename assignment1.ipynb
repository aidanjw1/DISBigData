{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "> **DO NOT EDIT IF INSIDE `computational_analysis_of_big_data_2017` folder** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handin in Peergrade**: *Wednesday, September 27, 2017*<br>\n",
    "**Peergrading deadline**: *Wednesday, October 4th, 2017*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 2.3.4**: You may have noticed that the data['data'] object has a key called 'after'.\n",
    "1. What do you think this is?\n",
    "2. Write a function that takes an integer `N` and the name of a subreddit, and returns a JSON with all posts on the first `N` pages of that subreddit. Use it to retrieve a large number of posts.\n",
    "3. Make an updated version of the figures you produced in Ex. 2.3.2-3 with this larger dataset.\n",
    "4. Visualize the number of posts over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-15104f793907>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mposts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mreddit_posts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubreddit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"news\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mreddit_posts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-15104f793907>\u001b[0m in \u001b[0;36msubreddit\u001b[0;34m(num_pages, name)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_pages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m#print data.keys()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpost\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'children'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mposts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mafter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'after'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'data'"
     ]
    }
   ],
   "source": [
    "# 1.\n",
    "# The key 'after' is a paramter that can be passed into the url as apointer to the next page which \n",
    "\n",
    "# 2.\n",
    "import requests as rq\n",
    "import json\n",
    "\n",
    "def subreddit(num_pages, name):\n",
    "    url = \"https://www.reddit.com/r/{0}/.json\".format(name)\n",
    "    data = rq.get(url).json()\n",
    "    \n",
    "    posts = []\n",
    "    \n",
    "    for i in range(num_pages):\n",
    "        #print data.keys()\n",
    "        for post in data['data']['children']:            \n",
    "            posts.append(post)\n",
    "        after = data['data']['after']\n",
    "        url = \"https://www.reddit.com/r/{0}/.json?after={1}\".format(name, after)\n",
    "        data = rq.get(url).json()\n",
    "        \n",
    "    return posts\n",
    "    \n",
    "reddit_posts = subreddit(3, \"news\")\n",
    "print reddit_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name = \"news\"\n",
    "url = \"https://www.reddit.com/r/{0}/.json\".format(name)\n",
    "data = rq.get(url).json()\n",
    "print data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3.\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import requests as rq\n",
    "\n",
    "comments = []\n",
    "scores = []\n",
    "\n",
    "for post in reddit_posts:\n",
    "    scores.append(post['data']['score'])\n",
    "    comments.append(post['data']['num_comments'])\n",
    "    \n",
    "plt.scatter(comments, scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 3.3.2**: Joel gives an example in the book that illustrates the conditional probablity of “both children are girls” knowing “at least one of the children is a girl” versus the probability that \"both children are girls\" knowing \"the older child is a girl\". He computes these probabilities with the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-15T14:23:13.638907Z",
     "start_time": "2017-09-15T14:23:13.599714Z"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def random_kid():\n",
    "    return random.choice([\"boy\", \"girl\"])\n",
    "\n",
    "both_girls = 0\n",
    "older_girl = 0\n",
    "either_girl = 0\n",
    "\n",
    "random.seed(0)\n",
    "for _ in range(10000):\n",
    "    younger = random_kid()\n",
    "    older = random_kid()\n",
    "    if older == \"girl\":\n",
    "        older_girl += 1\n",
    "    if older == \"girl\" and younger == \"girl\":\n",
    "        both_girls += 1\n",
    "    if older == \"girl\" or younger == \"girl\":\n",
    "        either_girl += 1\n",
    "\n",
    "print \"P(both | older):\", both_girls * 1.0 / older_girl      # 0.514 ~ 1/2\n",
    "print \"P(both | either): \", both_girls * 1.0 / either_girl   # 0.342 ~ 1/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Now imagine a family with three children. Assume the only genders are 'boy' and 'girl' and that their probability of occuring are equal and independent. Write a similar piece of code that computes:\n",
    "1. the probability of three girls?\n",
    "1. the probability of two girls and one boy?\n",
    "1. the probability of one girl and two boys?\n",
    "1. the probability of three boys?\n",
    "1. the probability that all children are girls given that the oldest child is a girl?\n",
    "1. the probability that all children are girls given that one of the children is a girl?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "allGirls = 0\n",
    "twoGirls = 0\n",
    "oneGirl = 0\n",
    "noGirls = 0\n",
    "eldestGirl = 0\n",
    "aGirl = 0\n",
    "\n",
    "random.seed(0)\n",
    "for _ in range(10000):\n",
    "    kids = [random_kid(), random_kid(), random_kid()]\n",
    "    isGirl = [kid == \"girl\" for kid in kids]\n",
    "    numGirls = 0\n",
    "    for girl in isGirl:\n",
    "        if girl:\n",
    "            numGirls += 1\n",
    "            \n",
    "    if numGirls == 3:\n",
    "        allGirls += 1\n",
    "    if numGirls == 2:\n",
    "        twoGirls += 1\n",
    "    if numGirls == 1:\n",
    "        oneGirl += 1\n",
    "    if numGirls == 0:\n",
    "        noGirls += 1\n",
    "    if numGirls > 0:\n",
    "        aGirl += 1\n",
    "    if isGirl[0]:\n",
    "        eldestGirl += 1\n",
    "        \n",
    "pAllGirls = allGirls * 1.0 / 10000\n",
    "pTwoGirls = twoGirls * 1.0 / 10000\n",
    "pOneGirl = oneGirl * 1.0 / 10000\n",
    "pNoGirls = noGirls * 1.0 / 10000\n",
    "pEldestGirl = eldestGirl * 1.0 / 10000\n",
    "pAGirl = aGirl * 1.0 / 10000\n",
    "\n",
    "print \"P(threeGirls) = \" + str(pAllGirls)\n",
    "print \"P(twoGirls) = \" + str(pTwoGirls)\n",
    "print \"P(oneGirl) = \" + str(pOneGirl)\n",
    "print \"P(noGirls) = \" + str(pNoGirls)\n",
    "print \"P(allGirls | eldestGirl) = \" + str(pAllGirls / pEldestGirl)\n",
    "print \"P(allGirls | aGirl) = \" + str(pAllGirls / pAGirl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 4.1.1**: From the Wikipedia API, get a list of all Marvel superheroes and another list of all Marvel supervillains. Use 'Category:Marvel_Comics_supervillains' and 'Category:Marvel_Comics_superheroes' to get the characters in each category.\n",
    "1. How many superheroes are there? How many supervillains?\n",
    "2. How many characters are both heroes and villains? What is the Jaccard similarity between the two groups?\n",
    "\n",
    ">*Hint: Google something like \"get list all pages in category wikimedia api\" if you're struggling with the query.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "965\n"
     ]
    }
   ],
   "source": [
    "#Code to find number of superheroes\n",
    "\n",
    "import requests as rq\n",
    "\n",
    "superHeroDict = rq.get(\"https://en.wikipedia.org/w/api.php?format=json&action=query&list=categorymembers&cmtitle=Category:Marvel_Comics_superheroes\").json()\n",
    "superHeros = []\n",
    "\n",
    "while(True):\n",
    "    for hero in superHeroDict[\"query\"][\"categorymembers\"]:\n",
    "            superHeros.append(hero[\"title\"])\n",
    "            \n",
    "    if \"continue\" in superHeroDict.keys():\n",
    "        cont = superHeroDict[\"continue\"][\"cmcontinue\"]\n",
    "        superHeroDict = rq.get(\"https://en.wikipedia.org/w/api.php?format=json&action=query&list=categorymembers&cmtitle=Category:Marvel_Comics_superheroes&cmcontinue=\" + cont).json()\n",
    "\n",
    "    else:\n",
    "        break\n",
    "    \n",
    "print len(superHeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1300\n"
     ]
    }
   ],
   "source": [
    "#Code to determine number of supervillains\n",
    "\n",
    "superVillainDict = rq.get(\"https://en.wikipedia.org/w/api.php?format=json&action=query&list=categorymembers&cmtitle=Category:Marvel_Comics_supervillains\").json()\n",
    "superVillains = []\n",
    "\n",
    "while(True):\n",
    "    for villain in superVillainDict[\"query\"][\"categorymembers\"]:\n",
    "            superVillains.append(villain[\"title\"])\n",
    "            \n",
    "    if \"continue\" in superVillainDict.keys():\n",
    "        cont = superVillainDict[\"continue\"][\"cmcontinue\"]\n",
    "        superVillainDict = rq.get(\"https://en.wikipedia.org/w/api.php?format=json&action=query&list=categorymembers&cmtitle=Category:Marvel_Comics_supervillains&cmcontinue=\" + cont).json()\n",
    "\n",
    "    else:\n",
    "        break\n",
    "\n",
    "print len(superVillains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0816618911175\n"
     ]
    }
   ],
   "source": [
    "#Code to find Jaccard Index\n",
    "\n",
    "#turn each list into a set\n",
    "heroSet = set(superHeros)\n",
    "villainSet = set(superVillains)\n",
    "\n",
    "#find the union and intersection between hero and villain set\n",
    "heroVillainUnion = heroSet.union(villainSet)\n",
    "heroVillainIntersection = heroSet.intersection(villainSet)\n",
    "\n",
    "#jaccard index\n",
    "jaccard = 1.0 * len(heroVillainIntersection) / len(heroVillainUnion)\n",
    "print jaccard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-15T14:24:53.339062Z",
     "start_time": "2017-09-15T14:24:53.334274Z"
    }
   },
   "source": [
    ">**Ex. 4.1.2**: Using this list you now want to download all data you can about each character. However, because this is potentially Big Data, you cannot store it your computer's memory. Therefore, you have to store it in your harddrive somehow. \n",
    "* Create three folders on your computer, one for *heroes*, one for *villains*, and one for *ambiguous*.\n",
    "* For each character, download the markdown on their pages and save in a new file in the corresponding hero/villain/ambiguous folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "villain\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "ambiguous\n",
      "villain\n",
      "villain\n",
      "ambiguous\n",
      "villain\n",
      "villain\n",
      "ambiguous\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "hero\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "ambiguous\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "hero\n",
      "hero\n",
      "hero\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "ambiguous\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "ambiguous\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "ambiguous\n",
      "ambiguous\n",
      "ambiguous\n",
      "hero\n",
      "hero\n",
      "hero\n",
      "ambiguous\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "ambiguous\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "hero\n",
      "hero\n",
      "hero\n",
      "ambiguous\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "ambiguous\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "ambiguous\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "ambiguous\n",
      "ambiguous\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "ambiguous\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "ambiguous\n",
      "hero\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "ambiguous\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "ambiguous\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "ambiguous\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "ambiguous\n",
      "hero\n",
      "villain\n",
      "ambiguous\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "ambiguous\n",
      "villain\n",
      "hero\n",
      "ambiguous\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "ambiguous\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "ambiguous\n",
      "ambiguous\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "ambiguous\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "ambiguous\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "ambiguous\n",
      "villain\n",
      "hero\n",
      "ambiguous\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "ambiguous\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "ambiguous\n",
      "hero\n",
      "hero\n",
      "hero\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "ambiguous\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "ambiguous\n",
      "hero\n",
      "ambiguous\n",
      "hero\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "hero\n",
      "ambiguous\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "hero\n",
      "hero\n",
      "ambiguous\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "ambiguous\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "ambiguous\n",
      "villain\n",
      "villain\n",
      "ambiguous\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "hero\n",
      "hero\n",
      "ambiguous\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "hero\n",
      "hero\n",
      "ambiguous\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "ambiguous\n",
      "villain\n",
      "villain\n",
      "ambiguous\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "ambiguous\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "ambiguous\n",
      "hero\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "ambiguous\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "ambiguous\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "ambiguous\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "ambiguous\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "ambiguous\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "hero\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "ambiguous\n",
      "villain\n",
      "ambiguous\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "ambiguous\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "ambiguous\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "hero\n",
      "ambiguous\n",
      "hero\n",
      "ambiguous\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "ambiguous\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "hero\n",
      "ambiguous\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "ambiguous\n",
      "villain\n",
      "villain\n",
      "ambiguous\n",
      "hero\n",
      "villain\n",
      "ambiguous\n",
      "villain\n",
      "villain\n",
      "ambiguous\n",
      "villain\n",
      "ambiguous\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "ambiguous\n",
      "hero\n",
      "villain\n",
      "ambiguous\n",
      "villain\n",
      "ambiguous\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "ambiguous\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "villain\n",
      "villain\n",
      "villain\n",
      "hero\n",
      "villain\n",
      "hero\n",
      "hero\n",
      "villain\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "\n",
    "for char in heroVillainUnion:\n",
    "    url = unicode(\"https://en.wikipedia.org/w/api.php?format=json&action=query&titles={0}&prop=revisions&rvprop=content\").format(char)\n",
    "    data = rq.get(url).json()\n",
    "    markup = data['query']['pages'].values()[0]['revisions'][0]['*']\n",
    "    \n",
    "    if char in heroVillainIntersection:\n",
    "        print \"ambiguous\"\n",
    "        filepath = \"./marvel/ambiguous/{0}.txt\".format(char)\n",
    "        filepath = filepath.strip(\"/\")\n",
    "        charfile = io.open(filepath, 'w')\n",
    "        charfile.write(markup)\n",
    "        charfile.close()\n",
    "    elif char in heroSet:\n",
    "        print \"hero\"\n",
    "        filepath = \"./marvel/heroes/{0}.txt\".format(char)\n",
    "        filepath = filepath.strip(\"/\")\n",
    "        charfile = io.open(filepath, 'w')\n",
    "        charfile.write(markup)\n",
    "        charfile.close()\n",
    "    else:\n",
    "        print \"villain\"\n",
    "        filepath = unicode(\"./marvel/villains/{0}.txt\").format(char)\n",
    "        filepath = filepath.strip(\"/\")\n",
    "        charfile = io.open(filepath, 'w')\n",
    "        charfile.write(markup)\n",
    "        charfile.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 4.2.1.1**: Extract the length of the page of each character, and plot the distribution of this variable for each class (heroes/villains/ambiguous). Can you say anything about the popularity of characters in the Marvel universe based on your visualization?\n",
    "\n",
    ">*Hint: The simplest thing is to make a probability mass function, i.e. a normalized histogram. Use `plt.hist` on a list of page lengths, with the argument `normed=True`. Other distribution plots are fine too, though.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 4.2.1.2**: Find the 10 characters from each class with the longest Wikipedia pages. Visualize their page lengths with bar charts. Comment on the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 4.2.2.1**: We are interested to know if there is a time-trend in the debut of characters.\n",
    "* Extract into three lists, debut years of heroes, villains, and ambiguous characters.\n",
    "* Do all pages have a debut year? Do some have multiple? How do you handle these inconsistencies?\n",
    "* Visualize the amount of heroes introduces over time. You choose how you want to visualize this data, but please comment on your choice. Also comment on the outcome of your analysis.\n",
    "\n",
    ">*Hint: The debut year is given on the debut row in the info table of a character's Wiki-page. There are many ways that you can extract this variable. You should try to have a go at it yourself, but if you are short on time, you can use this horribly ugly regular expression code:*\n",
    "\n",
    ">*`re.findall(r\"\\d{4}\\)\", re.findall(r\"debut.+?\\n\", markup_text)[0])[0][:-1]`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "heroes = []\n",
    "villains = []\n",
    "ambiguous = []\n",
    "\n",
    "for char in (heroVillainUnion):\n",
    "    if char in heroVillainIntersection:\n",
    "        ambiguous.append(char)\n",
    "    elif char in superHeros:\n",
    "        heroes.append(char)\n",
    "    elif char in superVillains:\n",
    "        villains.append(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "ambDebuts = []\n",
    "\n",
    "for char in ambiguous:\n",
    "    url = unicode(\"https://en.wikipedia.org/w/api.php?format=json&action=query&titles={0}&prop=revisions&rvprop=content\").format(char)\n",
    "    data = rq.get(url).json()\n",
    "    markup = data['query']['pages'].values()[0]['revisions'][0]['*']\n",
    "    \n",
    "    # Ignore anything without a debut date\n",
    "    try:\n",
    "        ambDebuts.append(int(re.findall(r\"\\d{4}\\)\", re.findall(r\"debut.+?\\n\", markup)[0])[0][:-1]))\n",
    "    except IndexError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "heroDebuts = []\n",
    "\n",
    "for char in heroes:\n",
    "    url = unicode(\"https://en.wikipedia.org/w/api.php?format=json&action=query&titles={0}&prop=revisions&rvprop=content\").format(char)\n",
    "    data = rq.get(url).json()\n",
    "    markup = data['query']['pages'].values()[0]['revisions'][0]['*']\n",
    "    \n",
    "    # Ignore anything without a debut date\n",
    "    try:\n",
    "        heroDebuts.append(int(re.findall(r\"\\d{4}\\)\", re.findall(r\"debut.+?\\n\", markup)[0])[0][:-1]))\n",
    "    except IndexError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "villainDebuts = []\n",
    "\n",
    "for char in villains:\n",
    "    url = unicode(\"https://en.wikipedia.org/w/api.php?format=json&action=query&titles={0}&prop=revisions&rvprop=content\").format(char)\n",
    "    data = rq.get(url).json()\n",
    "    markup = data['query']['pages'].values()[0]['revisions'][0]['*']\n",
    "    \n",
    "    # Ignore anything without a debut date\n",
    "    try:\n",
    "        villainDebuts.append(int(re.findall(r\"\\d{4}\\)\", re.findall(r\"debut.+?\\n\", markup)[0])[0][:-1]))\n",
    "    except IndexError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "ambDebutCounts = Counter(ambDebuts)\n",
    "heroDebutCounts = Counter(heroDebuts)\n",
    "villainDebutCounts = Counter(villainDebuts)\n",
    "\n",
    "print ambDebutCounts\n",
    "print heroDebutCounts\n",
    "print villainDebutCounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 4.2.3.1**: In this exercise we want to extract the team affiliations for each character. Each character may be associated with multiple teams. In the info table of the markup, the teams are listed in the *alliances*-field.\n",
    "* Write a regex extracts the *alliances*-field.\n",
    "* Write a regex that extracts each team from the *alliance*-field.\n",
    "* Count the number of members for each team (hint: use a `defaultdict`).\n",
    "* Inspect your team names. Are there any that result from inconsistencies in the information on the pages? How do you deal with this?\n",
    "* Print the 10 largest alliances and their number of members."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "dd = defaultdict(int)\n",
    "\n",
    "for char in heroVillainUnion:\n",
    "    url = unicode(\"https://en.wikipedia.org/w/api.php?format=json&action=query&titles={0}&prop=revisions&rvprop=content\").format(char)\n",
    "    data = rq.get(url).json()\n",
    "    markup = data['query']['pages'].values()[0]['revisions'][0]['*']\n",
    "\n",
    "    try:\n",
    "        alliances_text = re.findall(r\"alliances = .*.\", markup)[0]\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    alliances = re.findall(\"\\[\\[(.*?)\\]\\]\", alliances_text) \n",
    "    print alliances\n",
    "    print len(alliances)  \n",
    "    \n",
    "    for team in alliances:\n",
    "        dd[team] += 1\n",
    "        \n",
    "print dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print sorted(dd.values(), reverse=True)\n",
    "for item in sorted(dd.values(), reverse=True)[:10]:\n",
    "    print item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
